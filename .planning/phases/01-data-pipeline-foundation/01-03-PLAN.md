---
phase: 01-data-pipeline-foundation
plan: 03
type: execute
wave: 3
depends_on:
  - "01-01"
  - "01-02"
files_modified:
  - backend/app/scheduler.py
  - backend/app/main.py
  - backend/app/routers/__init__.py
  - backend/app/routers/health.py
autonomous: true
requirements:
  - DATA-01
  - DATA-02
  - DATA-03
  - DATA-05

must_haves:
  truths:
    - "APScheduler runs fetch_cycle every 5 minutes in background"
    - "On fetch failure, DB retains last-known-good rows (no delete on error)"
    - "GET /api/health returns last_fetched timestamp and status"
    - "Scheduler starts on app startup and shuts down cleanly on app stop"
    - "ScoreSnapshot rows are upserted (not duplicated) on each successful fetch"
  artifacts:
    - path: "backend/app/scheduler.py"
      provides: "APScheduler BackgroundScheduler + fetch_cycle function"
      exports: ["create_scheduler", "fetch_cycle"]
    - path: "backend/app/routers/health.py"
      provides: "GET /api/health endpoint"
      contains: "last_fetched"
    - path: "backend/app/main.py"
      provides: "Updated lifespan with scheduler start/stop + health router"
      contains: "scheduler.start()"
  key_links:
    - from: "backend/app/scheduler.py"
      to: "backend/app/services/data_fetcher.py"
      via: "fetch_cycle calls fetch_all_stocks()"
      pattern: "fetch_all_stocks"
    - from: "backend/app/scheduler.py"
      to: "backend/app/models/score_snapshot.py"
      via: "fetch_cycle writes ScoreSnapshot rows to DB"
      pattern: "ScoreSnapshot"
    - from: "backend/app/main.py"
      to: "backend/app/scheduler.py"
      via: "lifespan calls create_scheduler().start()"
      pattern: "create_scheduler"
    - from: "backend/app/main.py"
      to: "backend/app/routers/health.py"
      via: "app.include_router"
      pattern: "include_router.*health"
---

<objective>
Wire up APScheduler for 5-minute polling and expose the health check endpoint — completing the full data pipeline from scheduler trigger through DB write to API response.

Purpose: Activates the live data pipeline: scheduler calls fetcher, fetcher validates, results are persisted to DB, health endpoint surfaces last-updated timestamp (DATA-05).
Output: Running FastAPI app that polls Yahoo Finance every 5 minutes with fallback and health check.
</objective>

<execution_context>
@C:/Users/Yarin David/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Yarin David/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/01-data-pipeline-foundation/01-01-SUMMARY.md
@.planning/phases/01-data-pipeline-foundation/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: APScheduler fetch_cycle — 5-minute polling with DB upsert and fallback</name>
  <files>
    backend/app/scheduler.py
  </files>
  <action>
Create backend/app/scheduler.py. Use BackgroundScheduler (sync, not AsyncIOScheduler) because yfinance is a sync library — do NOT wrap in AsyncIOScheduler.

```python
import logging
from datetime import datetime, timezone
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
from sqlalchemy import create_engine, select
from sqlalchemy.orm import Session

from .config import settings
from .services.data_fetcher import fetch_all_stocks, SEED_TICKERS
from .models.score_snapshot import ScoreSnapshot
from .models.stock import Stock

logger = logging.getLogger(__name__)

# Sync engine for the scheduler thread (yfinance + DB writes are sync)
_sync_db_url = settings.database_url.replace("sqlite+aiosqlite", "sqlite")
_sync_engine = create_engine(_sync_db_url, connect_args={"check_same_thread": False})

def fetch_cycle() -> None:
    """Called by APScheduler every N minutes. Fetches data and persists to DB.
    On yfinance failure, returns without modifying DB — last-known-good data is retained."""
    logger.info("fetch_cycle: starting data fetch")
    data = fetch_all_stocks(SEED_TICKERS)
    if not data:
        logger.warning("fetch_cycle: no data returned, skipping DB write (last-known-good retained)")
        return

    now = datetime.now(timezone.utc)
    with Session(_sync_engine) as session:
        for ticker, values in data.items():
            snapshot = ScoreSnapshot(
                ticker=ticker,
                close_price=values["close_price"],
                volume=values["volume"],
                fetched_at=now,
            )
            session.add(snapshot)
            # Update last_updated on the Stock row
            stock = session.execute(select(Stock).where(Stock.ticker == ticker)).scalar_one_or_none()
            if stock:
                stock.last_updated = now
        session.commit()
    logger.info("fetch_cycle: persisted %d tickers at %s", len(data), now.isoformat())

def create_scheduler() -> BackgroundScheduler:
    """Creates and configures the APScheduler instance. Caller must call .start()."""
    scheduler = BackgroundScheduler(timezone="UTC")
    scheduler.add_job(
        fetch_cycle,
        IntervalTrigger(minutes=settings.fetch_interval_minutes),
        id="data_fetch",
        replace_existing=True,
        max_instances=1,  # Prevents overlapping runs if a fetch takes longer than the interval
    )
    return scheduler
```

Critical notes:
- `replace_existing=True` prevents duplicate jobs if scheduler restarts.
- `max_instances=1` prevents concurrent overlapping fetches.
- Sync SQLAlchemy engine uses `sqlite` driver (not `sqlite+aiosqlite`) — the scheduler runs in a background thread, not the async event loop.
- The fallback (DATA-03) is implicit: if fetch_all_stocks returns {}, we return early — existing ScoreSnapshot rows remain untouched.
  </action>
  <verify>
`python -c "from app.scheduler import create_scheduler, fetch_cycle; s = create_scheduler(); print('scheduler OK', s.get_jobs())"` — prints scheduler OK with one job listed.
  </verify>
  <done>create_scheduler() returns a BackgroundScheduler with one "data_fetch" job set to 5-minute interval. fetch_cycle() does not raise on import.</done>
</task>

<task type="auto">
  <name>Task 2: Health endpoint + wire scheduler into FastAPI lifespan</name>
  <files>
    backend/app/routers/__init__.py
    backend/app/routers/health.py
    backend/app/main.py
  </files>
  <action>
**backend/app/routers/__init__.py** — empty file.

**backend/app/routers/health.py** — GET /api/health returns last fetch timestamp:
```python
from datetime import datetime, timezone
from fastapi import APIRouter, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func

from ..database import get_db
from ..models.score_snapshot import ScoreSnapshot

router = APIRouter(prefix="/api", tags=["health"])

@router.get("/health")
async def health_check(db: AsyncSession = Depends(get_db)):
    """Returns system health and last successful data fetch timestamp."""
    # Query the most recent ScoreSnapshot fetched_at across all tickers
    result = await db.execute(select(func.max(ScoreSnapshot.fetched_at)))
    last_fetched = result.scalar_one_or_none()

    return {
        "status": "ok",
        "last_fetched": last_fetched.isoformat() if last_fetched else None,
        "data_available": last_fetched is not None,
        "timestamp": datetime.now(timezone.utc).isoformat(),
    }
```

**backend/app/main.py** — update the lifespan to start/stop scheduler, and register the health router:
```python
from contextlib import asynccontextmanager
from fastapi import FastAPI
from .database import engine, Base
from .scheduler import create_scheduler
from .routers.health import router as health_router

_scheduler = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global _scheduler
    # Initialize DB tables
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    # Seed initial data
    from .seed import seed_db
    await seed_db()
    # Start background scheduler
    _scheduler = create_scheduler()
    _scheduler.start()
    yield
    # Graceful shutdown
    if _scheduler:
        _scheduler.shutdown(wait=False)
    await engine.dispose()

app = FastAPI(title="Smart Stock Ranker", lifespan=lifespan)
app.include_router(health_router)

@app.get("/")
async def root():
    return {"status": "ok"}
```

Also create **backend/app/seed.py** (if not already created in plan 01-01):
```python
import logging
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from .database import async_session_maker
from .models.stock import Stock, Domain

logger = logging.getLogger(__name__)

SEED_DATA = {
    "AI/Tech": [
        ("AAPL", "Apple Inc."),
        ("MSFT", "Microsoft Corp."),
        ("NVDA", "NVIDIA Corp."),
        ("AMD", "Advanced Micro Devices"),
        ("GOOGL", "Alphabet Inc."),
    ],
    "EV": [
        ("TSLA", "Tesla Inc."),
        ("RIVN", "Rivian Automotive"),
    ],
    "Finance": [
        ("JPM", "JPMorgan Chase"),
        ("GS", "Goldman Sachs"),
    ],
}

async def seed_db():
    async with async_session_maker() as session:
        for domain_name, stocks in SEED_DATA.items():
            result = await session.execute(select(Domain).where(Domain.name == domain_name))
            domain = result.scalar_one_or_none()
            if not domain:
                domain = Domain(name=domain_name)
                session.add(domain)
                await session.flush()
            for ticker, name in stocks:
                result = await session.execute(select(Stock).where(Stock.ticker == ticker))
                stock = result.scalar_one_or_none()
                if not stock:
                    session.add(Stock(ticker=ticker, name=name, domain_id=domain.id))
        await session.commit()
    logger.info("seed_db: seed complete")
```
  </action>
  <verify>
1. `uvicorn app.main:app --port 8001` starts without errors (run from backend/ directory, Ctrl+C after confirming startup).
2. `curl http://localhost:8001/api/health` returns JSON with "status": "ok" and "last_fetched" key (may be null before first poll).
  </verify>
  <done>GET /api/health returns 200 with {"status": "ok", "last_fetched": ..., "data_available": ..., "timestamp": ...}. Scheduler starts on app startup (APScheduler log line visible). App shuts down cleanly on Ctrl+C.</done>
</task>

</tasks>

<verification>
Full pipeline verification:
1. `cd backend && uvicorn app.main:app --port 8001` — starts, no errors, APScheduler "Adding job..." log visible.
2. `curl http://localhost:8001/api/health` — returns 200 JSON with status/last_fetched/data_available/timestamp.
3. Wait 30 seconds after startup (or manually trigger fetch_cycle) — `curl http://localhost:8001/api/health` shows non-null last_fetched.
4. `curl http://localhost:8001/` — returns {"status": "ok"}.
</verification>

<success_criteria>
- uvicorn starts the app without import errors
- APScheduler is running with one "data_fetch" job (visible in logs)
- GET /api/health returns 200 with all four fields (status, last_fetched, data_available, timestamp)
- After one fetch cycle completes, last_fetched is a valid ISO timestamp
- App shuts down gracefully (no hanging threads)
- DATA-05: last_fetched timestamp is queryable via /api/health
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline-foundation/01-03-SUMMARY.md` using the summary template.
</output>
