---
phase: 05-historical-tracking-custom-domains
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/models/daily_snapshot.py
  - backend/app/services/snapshot_service.py
  - backend/app/scheduler.py
  - backend/alembic/versions/05_01_daily_snapshots.py
autonomous: true
requirements: [HIST-01]

must_haves:
  truths:
    - "daily_snapshots table exists in Supabase with correct schema"
    - "snapshot_job runs at 21:00 UTC and writes one row per ticker per date"
    - "trend_slope is computed from last 7 days of scores using np.polyfit"
    - "duplicate runs on same date upsert (no duplicate rows)"
  artifacts:
    - path: "backend/app/models/daily_snapshot.py"
      provides: "DailySnapshot SQLAlchemy model"
      contains: "class DailySnapshot"
    - path: "backend/app/services/snapshot_service.py"
      provides: "snapshot_job + compute_trend"
      exports: ["snapshot_job", "compute_trend"]
    - path: "backend/alembic/versions/05_01_daily_snapshots.py"
      provides: "Alembic migration for daily_snapshots table"
  key_links:
    - from: "backend/app/scheduler.py"
      to: "backend/app/services/snapshot_service.py"
      via: "import + add_job(snapshot_job, CronTrigger)"
      pattern: "snapshot_job"
    - from: "backend/app/services/snapshot_service.py"
      to: "backend/app/models/daily_snapshot.py"
      via: "Session upsert using DailySnapshot model"
      pattern: "DailySnapshot"
---

<objective>
Create the DailySnapshot model, Alembic migration, compute_trend utility, and snapshot_job scheduler job.

Purpose: Persist daily score snapshots at EOD so history charts and trend indicators have data to display.
Output: daily_snapshots table in Supabase, nightly cron job writing one row per ticker.
</objective>

<execution_context>
@C:/Users/Yarin David/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Yarin David/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@backend/app/models/score_snapshot.py
@backend/app/scheduler.py
@backend/app/services/data_fetcher.py
@backend/alembic/versions/81cb55ab1baf_initial_schema.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: DailySnapshot model + Alembic migration</name>
  <files>backend/app/models/daily_snapshot.py, backend/alembic/versions/05_01_daily_snapshots.py</files>
  <action>
Create `backend/app/models/daily_snapshot.py` using SQLAlchemy 2.0 `Mapped[T]` / `mapped_column` syntax:

```python
class DailySnapshot(Base):
    __tablename__ = "daily_snapshots"
    ticker: Mapped[str] = mapped_column(String(10), primary_key=True)
    snap_date: Mapped[date] = mapped_column(Date, primary_key=True)
    composite_score: Mapped[float] = mapped_column(Float, nullable=False)
    rank: Mapped[int] = mapped_column(Integer, nullable=False)
    domain_id: Mapped[Optional[int]] = mapped_column(Integer, ForeignKey("domains.id"), nullable=True)
    trend_slope: Mapped[float] = mapped_column(Float, nullable=False, server_default="0.0")
```

Create Alembic migration `backend/alembic/versions/05_01_daily_snapshots.py`:
- `op.create_table("daily_snapshots", ...)` with all columns + composite PK (ticker, snap_date)
- FK to domains.id (nullable)
- Do NOT use autogenerate — write upgrade/downgrade manually
- downgrade: `op.drop_table("daily_snapshots")`

Run: `cd backend && alembic upgrade head`
  </action>
  <verify>
    cd backend && alembic upgrade head
    python -c "from app.models.daily_snapshot import DailySnapshot; print('OK')"
  </verify>
  <done>Migration applies without error; DailySnapshot importable from app.models.daily_snapshot</done>
</task>

<task type="auto">
  <name>Task 2: snapshot_service.py + wire into scheduler</name>
  <files>backend/app/services/snapshot_service.py, backend/app/scheduler.py</files>
  <action>
Create `backend/app/services/snapshot_service.py`:

```python
import numpy as np
from datetime import date, timedelta
from sqlalchemy.orm import Session
from app.models.daily_snapshot import DailySnapshot
# import RankingResult from wherever it lands in Phase 3
# For now import from app.models.ranking_result — if not yet existing, add TODO comment

def compute_trend(scores: list[float]) -> float:
    """Linear slope over scores list. Returns 0.0 if fewer than 2 points."""
    if len(scores) < 2:
        return 0.0
    x = np.arange(len(scores), dtype=float)
    slope, _ = np.polyfit(x, scores, 1)
    return float(slope)

def snapshot_job() -> None:
    """
    EOD job: reads latest RankingResult per ticker, writes/updates DailySnapshot for today.
    Uses sync engine (_sync_engine) — runs in APScheduler BackgroundScheduler thread.
    """
    from app.scheduler import _sync_engine  # avoid circular import at module level
    from app.models.ranking_result import RankingResult  # Phase 3 model

    today = date.today()
    with Session(_sync_engine) as session:
        # Get latest ranking result per ticker
        results = session.execute(
            # subquery: max fetched_at per ticker, join back to get full row
            ...
        ).scalars().all()

        for result in results:
            # Get last 7 DailySnapshot scores for this ticker (excluding today)
            past = session.query(DailySnapshot.composite_score)\
                .filter(
                    DailySnapshot.ticker == result.ticker,
                    DailySnapshot.snap_date >= today - timedelta(days=7),
                    DailySnapshot.snap_date < today,
                )\
                .order_by(DailySnapshot.snap_date)\
                .all()
            past_scores = [row[0] for row in past]
            slope = compute_trend(past_scores + [result.composite_score])

            # Upsert: merge handles insert-or-update on composite PK
            snap = session.merge(DailySnapshot(
                ticker=result.ticker,
                snap_date=today,
                composite_score=result.composite_score,
                rank=result.rank,
                domain_id=result.domain_id,
                trend_slope=slope,
            ))

        session.commit()
```

NOTE: RankingResult model is created in Phase 3 (03-01-PLAN). If it does not exist yet, add a clear `# TODO: import RankingResult once Phase 3 is complete` comment and stub the query body with `results = []` so the file is importable. Do not block on missing Phase 3 model.

Wire into `backend/app/scheduler.py`:
- `from app.services.snapshot_service import snapshot_job`
- `from apscheduler.triggers.cron import CronTrigger`
- Add: `scheduler.add_job(snapshot_job, CronTrigger(hour=21, minute=0, timezone="UTC"), id="snapshot_job")`
- Add after the existing fetch_cycle job registration.
  </action>
  <verify>
    cd backend && python -c "from app.services.snapshot_service import snapshot_job, compute_trend; print(compute_trend([50.0, 55.0, 60.0]))"
    # Expected: positive float ~5.0
  </verify>
  <done>compute_trend returns correct slope; snapshot_service importable; scheduler registers snapshot_job at 21:00 UTC</done>
</task>

</tasks>

<verification>
- `alembic upgrade head` succeeds
- `daily_snapshots` table visible in Supabase dashboard
- `from app.services.snapshot_service import compute_trend` works
- `compute_trend([50.0, 55.0, 60.0])` returns ~5.0
- `compute_trend([42.0])` returns 0.0
- scheduler.py registers `snapshot_job` with CronTrigger(hour=21)
</verification>

<success_criteria>
daily_snapshots table in Supabase; snapshot_job registered in scheduler; compute_trend correct for 0, 1, N points.
</success_criteria>

<output>
After completion, create `.planning/phases/05-historical-tracking-custom-domains/05-01-SUMMARY.md`
</output>
